{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bwool/RESEARCH/TRB-Home-Data-Quality-2025'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "from geopy.distance import great_circle\n",
    "import traceback\n",
    "\n",
    "os.chdir(\"../..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cleaning 2019 Mobile Location Data\n",
    "\n",
    "##  Overview\n",
    "\n",
    "This notebook processes raw mobile location data from 2019 by:\n",
    "- Removing duplicate and low-accuracy observations\n",
    "- Deduplicating spatially within 30-minute bins\n",
    "- Adding derived features\n",
    "- Saving cleaned parquet files\n",
    "\n",
    "üì¶ Raw data size: **~7.5 GB**  \n",
    "üìÅ Cleaned data size: **~2.7 GB**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Processing Flow\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Read Parquet File] --> B[Sort by caid + utc_timestamp]\n",
    "    B --> C[Convert utc_timestamp to datetime_pdt]\n",
    "    C --> D[Drop low-accuracy pings (accuracy > 50m)]\n",
    "    D --> E[Compute speed & acceleration per user]\n",
    "    E --> F[Drop implausible speed (>50 m/s) or accel (>10 m/s¬≤)]\n",
    "    F --> G[Drop duplicate timestamps per user]\n",
    "    G --> H[Drop duplicate location pings per 30-min bin (rounded lat/lon)]\n",
    "    H --> I[Drop users with <10 observations]\n",
    "    I --> J[Compute time_diff_minutes]\n",
    "    J --> K[Map id_type to is_iOS]\n",
    "    K --> L[Drop unneeded columns]\n",
    "    L --> M[Reorder columns]\n",
    "    M --> N[Write cleaned parquet]    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to remove implausible points\n",
    "def compute_speed_accel(df):\n",
    "    df = df.sort_values('datetime_pdt')\n",
    "    coords = df[['latitude', 'longitude']].to_numpy()\n",
    "    times = (df['datetime_pdt'].astype('int64') / 1e9).to_numpy()  # seconds\n",
    "\n",
    "    speeds = [0]\n",
    "    accels = [0]\n",
    "    for i in range(1, len(df)):\n",
    "        d = great_circle(coords[i], coords[i-1]).meters\n",
    "        dt = times[i] - times[i-1]\n",
    "        if dt <= 0:\n",
    "            speeds.append(speeds[-1])\n",
    "            accels.append(0)\n",
    "            continue\n",
    "        speed = d / dt\n",
    "        accel = (speed - speeds[-1]) / dt\n",
    "        speeds.append(speed)\n",
    "        accels.append(accel)\n",
    "\n",
    "    df['speed_mps'] = speeds\n",
    "    df['accel_mps2'] = accels\n",
    "    return df\n",
    "\n",
    "def process_parquet(filepath:str, output_filepath:str=None) -> pd.DataFrame:\n",
    "    # === Load & sort ===\n",
    "    data= pd.read_parquet(filepath, engine='pyarrow')\n",
    "    data = data.sort_values(['caid', 'utc_timestamp'], kind='mergesort')\n",
    "\n",
    "    # === Drop users with less than 10 observations ===\n",
    "    valid_users = data['caid'].value_counts()[lambda x: x >= 10].index\n",
    "    data = data[data['caid'].isin(valid_users)]\n",
    "\n",
    "    # === Timestamps ===\n",
    "    data['datetime_utc'] = pd.to_datetime(data['utc_timestamp'], unit='s', utc=True)\n",
    "    data['datetime_pdt'] = data['datetime_utc'].dt.tz_convert('America/Los_Angeles')\n",
    "\n",
    "    # === Remove low-accuracy observations ===\n",
    "    data = data[data['horizontal_accuracy'] <= 50]\n",
    "    \n",
    "    # === Remove improbable observations ===\n",
    "    \"\"\"data = data.groupby('caid', group_keys=False).apply(compute_speed_accel)\n",
    "    data = data[(data['speed_mps'] <= 50) & (data['accel_mps2'].abs() <= 10)]\"\"\"\n",
    "\n",
    "    # === Remove duplicate timestamps ===\n",
    "    data = data.drop_duplicates(subset=[\"caid\", \"datetime_pdt\"])\n",
    "\n",
    "    # === Remove duplicate location recordings per 30-minute bin ===\n",
    "    data['time_bin'] = data['datetime_pdt'].dt.floor('30min')\n",
    "    #  round lat/lon for fuzzy deduplication (4th decimal ~8.5m)\n",
    "    data['lat_bin'] = data['latitude'].round(4)\n",
    "    data['lon_bin'] = data['longitude'].round(4)\n",
    "    data = data.sort_values('datetime_pdt').drop_duplicates(subset=['caid', 'time_bin', 'lat_bin', 'lon_bin'])\n",
    "    data = data.drop(columns=['time_bin', 'lat_bin', 'lon_bin'])\n",
    "\n",
    "    # === Again, Drop users with less than 10 observations ===\n",
    "    valid_users = data['caid'].value_counts()[lambda x: x >= 10].index\n",
    "    data = data[data['caid'].isin(valid_users)]\n",
    "\n",
    "    # === Compute time difference in minutes within the same user ===\n",
    "    data[\"time_diff_minutes\"] = data.groupby(\"caid\")[\"datetime_pdt\"].diff().shift(-1) / 60\n",
    "\n",
    "    # === Encode device type ===\n",
    "    data['is_iOS'] = data['id_type'].map({'idfa': True, 'aaid': False})\n",
    "\n",
    "    # === Drop uneccesary columns===\n",
    "    data = data.drop(columns=['id_type', 'geo_hash', 'altitude', 'iso_country_code', 'utc_timestamp', 'datetime_utc'])\n",
    "\n",
    "    # === Reorder ===\n",
    "    new_order = ['caid', 'datetime_pdt', 'latitude', 'longitude', 'is_iOS', 'time_diff_minutes', 'horizontal_accuracy', 'ip_address']\n",
    "    data = data[new_order]\n",
    "\n",
    "    # === Save output ===\n",
    "    if output_filepath:\n",
    "        data.to_parquet(output_filepath, engine='pyarrow', index=False)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Run Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2019 files:  11%|‚ñà         | 1/9 [00:48<06:27, 48.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00001-85a0c7d9-db42-457d-ab9d-d104038b7a1e-c000.snappy.parquet: 0.788 GB -> 0.310 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2019 files:  22%|‚ñà‚ñà‚ñè       | 2/9 [01:36<05:39, 48.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00002-85a0c7d9-db42-457d-ab9d-d104038b7a1e-c000.snappy.parquet: 0.761 GB -> 0.299 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2019 files:  33%|‚ñà‚ñà‚ñà‚ñé      | 3/9 [02:23<04:46, 47.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00003-85a0c7d9-db42-457d-ab9d-d104038b7a1e-c000.snappy.parquet: 0.771 GB -> 0.296 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2019 files:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 4/9 [03:13<04:02, 48.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-85a0c7d9-db42-457d-ab9d-d104038b7a1e-c000.snappy.parquet: 0.782 GB -> 0.301 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2019 files:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 5/9 [04:02<03:15, 48.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00005-85a0c7d9-db42-457d-ab9d-d104038b7a1e-c000.snappy.parquet: 0.786 GB -> 0.308 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2019 files:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6/9 [04:52<02:27, 49.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00007-85a0c7d9-db42-457d-ab9d-d104038b7a1e-c000.snappy.parquet: 0.782 GB -> 0.311 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2019 files:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 7/9 [05:40<01:37, 48.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00004-85a0c7d9-db42-457d-ab9d-d104038b7a1e-c000.snappy.parquet: 0.779 GB -> 0.306 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2019 files:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8/9 [06:29<00:48, 48.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00008-85a0c7d9-db42-457d-ab9d-d104038b7a1e-c000.snappy.parquet: 0.767 GB -> 0.295 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2019 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [07:19<00:00, 48.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00006-85a0c7d9-db42-457d-ab9d-d104038b7a1e-c000.snappy.parquet: 0.777 GB -> 0.302 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_2019_data():\n",
    "    # === Set up folder and file list ===\n",
    "    folder_2019 = \"00_Data/01_Sample_Data/2019_Sample_Data\"\n",
    "    output_folder = \"00_Data/02_Cleaned_Sample_Data/2019_Cleaned_Data\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    raw_files_2019 = [os.path.join(folder_2019, f) for f in os.listdir(folder_2019) if f.endswith(\".parquet\")]\n",
    "\n",
    "\n",
    "    # === Process files with progress tracking ===\n",
    "    for filepath in tqdm(raw_files_2019, desc=\"Processing 2019 files\"):\n",
    "        try:\n",
    "            # Compute original file size\n",
    "            original_size_bytes = os.path.getsize(filepath)\n",
    "\n",
    "            # Build output path\n",
    "            filename = os.path.basename(filepath)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            # Process and save cleaned file\n",
    "            process_parquet(filepath, output_filepath=output_path)\n",
    "\n",
    "            # Compute cleaned file size\n",
    "            cleaned_size_bytes = os.path.getsize(output_path)\n",
    "\n",
    "            # Report sizes\n",
    "            original_gb = original_size_bytes / (1024 ** 3)\n",
    "            cleaned_gb = cleaned_size_bytes / (1024 ** 3)\n",
    "\n",
    "            print(f\"{filename}: {original_gb:.3f} GB -> {cleaned_gb:.3f} GB\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {filepath}: {type(e).__name__}: {e}\")\n",
    "            traceback.print_exc()            \n",
    "\n",
    "process_2019_data()\n",
    "# 7m 19.1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Comparison: Raw vs Cleaned\n",
    "\n",
    "| **Field Name**           | **Kept?** | **Renamed / Derived?**     | **Notes**                                      |\n",
    "|--------------------------|-----------|-----------------------------|------------------------------------------------|\n",
    "| `caid`                   | ‚úÖ        | ‚Äî                           | Unique user ID                                 |\n",
    "| `utc_timestamp`          | ‚ùå        | ‚û° `datetime_pdt`           | Converted to timezone datetime (PDT)           |\n",
    "| `id_type`                | ‚ùå        | ‚û° `is_iOS`                 | Encoded to boolean                             |\n",
    "| `geo_hash`               | ‚ùå        | ‚Äî                           | Dropped                                        |\n",
    "| `latitude`               | ‚úÖ        | ‚Äî                           | Retained                                       |\n",
    "| `longitude`              | ‚úÖ        | ‚Äî                           | Retained                                       |\n",
    "| `horizontal_accuracy`    | ‚úÖ        | ‚Äî                           | Retained (filtered: must be ‚â§ 50 m)                                     |\n",
    "| `ip_address`             | ‚úÖ        | ‚Äî                           | Retained                                       |\n",
    "| `altitude`               | ‚ùå        | ‚Äî                           | Dropped                                        |\n",
    "| `iso_country_code`       | ‚ùå        | ‚Äî                           | Dropped                                        |\n",
    "| *(derived)*              | ‚úÖ        | `time_diff_minutes`        | Time diff to next observation (same user)      |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
