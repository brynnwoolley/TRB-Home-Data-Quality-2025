{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bwool/RESEARCH/TRB-Home-Data-Quality-2025'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir(\"../..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cleaning 2019 Mobile Location Data\n",
    "\n",
    "##  Overview\n",
    "\n",
    "This notebook processes raw mobile location data from 2019 by:\n",
    "- Removing duplicates and noise\n",
    "- Keeping only valid users\n",
    "- Adding derived features\n",
    "- Saving cleaned parquet files\n",
    "\n",
    "üì¶ Raw data size: **~7.5 GB**  \n",
    "üìÅ Cleaned data size: **~4.7 GB**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Processing Flow\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Read Parquet File] --> B[Sort by caid + utc_timestamp]\n",
    "    B --> C[Drop duplicate timestamps]\n",
    "    C --> D[Keep only start/end of identical lat-lon runs]\n",
    "    D --> E[Drop users with <10 observations]\n",
    "    E --> F[Compute time_diff_minutes]\n",
    "    F --> G[Convert utc_timestamp to datetime_pdt]\n",
    "    G --> H[Map id_type to is_iOS]\n",
    "    H --> I[Drop unneeded columns]\n",
    "    I --> J[Reorder columns]\n",
    "    J --> K[Write cleaned parquet]\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_parquet(filepath:str, output_filepath:str=None) -> pd.DataFrame:\n",
    "    # === Load & sort ===\n",
    "    data= pd.read_parquet(filepath, engine='pyarrow')\n",
    "    data = data.sort_values(['caid', 'utc_timestamp'], kind='mergesort')\n",
    "\n",
    "    # === Remove duplicate timestamps ===\n",
    "    data = data.drop_duplicates(subset=[\"caid\", \"utc_timestamp\"])\n",
    "\n",
    "    # === Remove consecutive duplicate locations, keeping first & last ===\n",
    "    lat = data['latitude'].to_numpy()\n",
    "    lon = data['longitude'].to_numpy()\n",
    "    caid = data['caid'].to_numpy()\n",
    "\n",
    "    # Find run starts: lat/lon/caid != previous\n",
    "    diff_prev = (\n",
    "        (lat != np.roll(lat, 1)) |\n",
    "        (lon != np.roll(lon, 1)) |\n",
    "        (caid != np.roll(caid, 1))\n",
    "    )\n",
    "    diff_prev[0] = True  # always keep first row\n",
    "\n",
    "    # Find run ends: lat/lon/caid != next\n",
    "    diff_next = (\n",
    "        (lat != np.roll(lat, -1)) |\n",
    "        (lon != np.roll(lon, -1)) |\n",
    "        (caid != np.roll(caid, -1))\n",
    "    )\n",
    "    diff_next[-1] = True  # always keep last row\n",
    "    mask = diff_prev | diff_next  # Keep if start or end of run\n",
    "\n",
    "    # Filter in place\n",
    "    data = data.loc[mask]\n",
    "\n",
    "    # === Drop users with less than 10 observations ===\n",
    "    valid_users = data['caid'].value_counts()[lambda x: x >= 10].index\n",
    "    data = data[data['caid'].isin(valid_users)]\n",
    "\n",
    "    # === Compute time difference in minutes within the same user ===\n",
    "    data[\"time_diff_minutes\"] = data.groupby(\"caid\")[\"utc_timestamp\"].diff().shift(-1) / 60\n",
    "\n",
    "    # === Timestamps ===\n",
    "    data['datetime_utc'] = pd.to_datetime(data['utc_timestamp'], unit='s', utc=True)\n",
    "    data['datetime_pdt'] = data['datetime_utc'].dt.tz_convert('America/Los_Angeles')\n",
    "\n",
    "    # === Encode device type ===\n",
    "    data['is_iOS'] = data['id_type'].map({'idfa': True, 'aaid': False})\n",
    "\n",
    "    # === Drop uneccesary columns===\n",
    "    data = data.drop(columns=['id_type', 'geo_hash', 'altitude', 'iso_country_code', 'utc_timestamp', 'datetime_utc'])\n",
    "\n",
    "    # === Reorder ===\n",
    "    new_order = ['caid', 'datetime_pdt', 'latitude', 'longitude', 'is_iOS', 'time_diff_minutes', 'horizontal_accuracy', 'ip_address']\n",
    "    data = data[new_order]\n",
    "\n",
    "    # === Save output ===\n",
    "    if output_filepath:\n",
    "        data.to_parquet(output_filepath, engine='pyarrow', index=False)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Run Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_2019_data():\n",
    "    # === Set up folder and file list ===\n",
    "    folder_2019 = \"00_Data/01_Sample_Data/2019_Sample_Data\"\n",
    "    output_folder = \"00_Data/01_Sample_Data/2019_Cleaned_Data\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    raw_files_2019 = [os.path.join(folder_2019, f) for f in os.listdir(folder_2019) if f.endswith(\".parquet\")]\n",
    "\n",
    "\n",
    "    # === Process files with progress tracking ===\n",
    "    for filepath in tqdm(raw_files_2019, desc=\"Processing 2019 files\"):\n",
    "        try:\n",
    "            # Compute original file size\n",
    "            original_size_bytes = os.path.getsize(filepath)\n",
    "\n",
    "            # Build output path\n",
    "            filename = os.path.basename(filepath)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            # Process and save cleaned file\n",
    "            process_parquet(filepath, output_filepath=output_path)\n",
    "\n",
    "            # Compute cleaned file size\n",
    "            cleaned_size_bytes = os.path.getsize(output_path)\n",
    "\n",
    "            # Report sizes\n",
    "            original_gb = original_size_bytes / (1024 ** 3)\n",
    "            cleaned_gb = cleaned_size_bytes / (1024 ** 3)\n",
    "\n",
    "            print(f\"{filename}: {original_gb:.3f} GB -> {cleaned_gb:.3f} GB\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filepath}: {e}\")\n",
    "\n",
    "#process_2019_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Comparison: Raw vs Cleaned\n",
    "\n",
    "| **Field Name**           | **Kept?** | **Renamed / Derived?**     | **Notes**                                      |\n",
    "|--------------------------|-----------|-----------------------------|------------------------------------------------|\n",
    "| `caid`                   | ‚úÖ        | ‚Äî                           | Unique user ID                                 |\n",
    "| `utc_timestamp`          | ‚ùå        | ‚û° `datetime_pdt`           | Converted to timezone-aware datetime           |\n",
    "| `id_type`                | ‚ùå        | ‚û° `is_iOS`                 | Encoded to boolean                             |\n",
    "| `geo_hash`               | ‚ùå        | ‚Äî                           | Dropped                                        |\n",
    "| `latitude`               | ‚úÖ        | ‚Äî                           | Retained                                       |\n",
    "| `longitude`              | ‚úÖ        | ‚Äî                           | Retained                                       |\n",
    "| `horizontal_accuracy`    | ‚úÖ        | ‚Äî                           | Retained                                       |\n",
    "| `ip_address`             | ‚úÖ        | ‚Äî                           | Retained                                       |\n",
    "| `altitude`               | ‚ùå        | ‚Äî                           | Dropped                                        |\n",
    "| `iso_country_code`       | ‚ùå        | ‚Äî                           | Dropped                                        |\n",
    "| *(derived)*              | ‚úÖ        | `time_diff_minutes`        | Time diff to next observation (same user)      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Core metrics ===\n",
    "total_records = data.groupby('caid').size().rename('total_records')\n",
    "\n",
    "prop_ios = data.groupby('caid')['is_iOS'].mean().rename('prop_ios').astype(float)\n",
    "\n",
    "data['date'] = data['datetime_pdt'].dt.date\n",
    "days_with_data = data.groupby('caid')['date'].nunique().rename('days_with_data')\n",
    "\n",
    "prop_high_accuracy = (\n",
    "    data.groupby('caid')['horizontal_accuracy']\n",
    "    .apply(lambda x: (x <= 100).mean())\n",
    "    .rename('prop_high_accuracy')\n",
    ")\n",
    "\n",
    "# === Night & bin labels ON MAIN DATA ===\n",
    "data['hour'] = data['datetime_pdt'].dt.hour\n",
    "data['minute'] = data['datetime_pdt'].dt.minute\n",
    "\n",
    "data['is_night'] = ((data['hour'] >= 19) | (data['hour'] < 7))\n",
    "\n",
    "data['night_date'] = data['datetime_pdt'].dt.date\n",
    "data.loc[data['hour'] < 7, 'night_date'] -= pd.Timedelta(days=1)\n",
    "\n",
    "# Only bin night pings\n",
    "mask_night = data['is_night']\n",
    "data.loc[mask_night, 'night_minute'] = data.loc[mask_night, 'hour'] * 60 + data.loc[mask_night, 'minute']\n",
    "data.loc[mask_night, 'night_bin'] = data.loc[mask_night, 'night_minute'] // 30\n",
    "\n",
    "# === Night metrics ===\n",
    "temp = data.loc[mask_night, ['caid', 'night_date', 'night_bin']].copy()\n",
    "\n",
    "# Total night pings & unique nights\n",
    "night_counts = (\n",
    "    temp.groupby('caid')\n",
    "    .agg(\n",
    "        total_night_pings=('night_date', 'count'),\n",
    "        unique_nights=('night_date', 'nunique')\n",
    "    )\n",
    ")\n",
    "night_counts['avg_night_pings_per_night'] = (\n",
    "    night_counts['total_night_pings'] / night_counts['unique_nights']\n",
    ")\n",
    "\n",
    "# === Bin-level stats ===\n",
    "# Bins per night per user-night\n",
    "bins_per_night = (\n",
    "    temp.groupby(['caid', 'night_date'])['night_bin']\n",
    "    .nunique()\n",
    "    .rename('bins_this_night')\n",
    ").reset_index()\n",
    "\n",
    "# Average bins per night\n",
    "avg_bins_per_night = (\n",
    "    bins_per_night.groupby('caid')['bins_this_night']\n",
    "    .mean()\n",
    "    .rename('avg_bins_per_night')\n",
    ")\n",
    "\n",
    "# === Combine all ===\n",
    "user_metrics = pd.concat([\n",
    "    total_records,\n",
    "    prop_ios,\n",
    "    days_with_data,\n",
    "    prop_high_accuracy\n",
    "], axis=1).reset_index()\n",
    "\n",
    "user_metrics = (\n",
    "    user_metrics\n",
    "    .merge(night_counts.reset_index(), on='caid', how='left')\n",
    "    .merge(avg_bins_per_night.reset_index(), on='caid', how='left')\n",
    ")\n",
    "\n",
    "# === Save ===\n",
    "filepath = '00_Sample_Data/2019_Pre_HDA_Metrics.csv'\n",
    "user_metrics.to_csv(filepath, index=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Data Quality Feature Descriptions\n",
    "\n",
    "| Field Name                | Description |\n",
    "|---------------------------|-------------|\n",
    "| caid                      | Unique user ID |\n",
    "| total_records             | Total number of records (rows) per user |\n",
    "| prop_ios                  | Proportion of records where device type is iOS |\n",
    "| days_with_data            | Number of unique days with at least one record |\n",
    "| prop_high_accuracy        | Proportion of records with horizontal accuracy ‚â§ 100 meters |\n",
    "| total_night_pings         | Total number of night-time records (7‚ÄØPM‚Äì7‚ÄØAM) |\n",
    "| unique_nights             | Number of unique nights with at least one night-time record |\n",
    "| avg_night_pings_per_night | Average number of night-time records per unique night |\n",
    "| avg_bins_per_night        | Average number of unique 30-minute bins per night |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# home inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Custom Meanshift ===\n",
    "class MeanShift:\n",
    "    def __init__(self, bandwidth, bin_seeding=True, min_bin_freq=2, max_iter=50):\n",
    "        self.bandwidth = bandwidth\n",
    "        self.bin_seeding = bin_seeding\n",
    "        self.min_bin_freq = min_bin_freq\n",
    "        self.max_iter = max_iter\n",
    "        self.cluster_center = None\n",
    "        self.used_mean = False\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bin_seeds(X, bin_size, min_bin_freq=1):\n",
    "        bin_sizes = defaultdict(int)\n",
    "        for point in X:\n",
    "            binned = np.round(point / bin_size)\n",
    "            bin_sizes[tuple(binned)] += 1\n",
    "        seeds = [np.array(point) * bin_size for point, freq in bin_sizes.items() if freq >= min_bin_freq]\n",
    "        return np.array(seeds) if seeds else X\n",
    "\n",
    "    @staticmethod\n",
    "    def fit_single_seed(seed, X, nbrs, bandwidth, max_iter):\n",
    "        stop_thresh = 1e-3 * bandwidth\n",
    "        mean = seed\n",
    "        for _ in range(max_iter):\n",
    "            indices = nbrs.radius_neighbors([mean], bandwidth, return_distance=False)[0]\n",
    "            if len(indices) == 0:\n",
    "                break\n",
    "            old_mean = mean\n",
    "            mean = X[indices].mean(axis=0)\n",
    "            if np.linalg.norm(mean - old_mean) < stop_thresh:\n",
    "                break\n",
    "        return tuple(mean), len(indices)\n",
    "\n",
    "    def fit(self, X):\n",
    "        if self.bin_seeding:\n",
    "            seeds = self.get_bin_seeds(X, self.bandwidth, self.min_bin_freq)\n",
    "        else:\n",
    "            seeds = X\n",
    "\n",
    "        nbrs = NearestNeighbors(radius=self.bandwidth).fit(X)\n",
    "        results = [self.fit_single_seed(seed, X, nbrs, self.bandwidth, self.max_iter) for seed in seeds]\n",
    "\n",
    "        clusters = {center: size for center, size in results if size > 0}\n",
    "\n",
    "        if not clusters:\n",
    "            self.cluster_center = tuple(X.mean(axis=0))\n",
    "            self.used_mean = True\n",
    "            return self\n",
    "\n",
    "        self.cluster_center = max(clusters.items(), key=lambda x: x[1])[0]\n",
    "        return self\n",
    "\n",
    "\n",
    "# === Load Data ===\n",
    "data = pd.read_parquet('00_Sample_Data/2019_Pre_HDA_Data.parquet')\n",
    "\n",
    "# === Prepare Superpings ===\n",
    "slot_size = 30 * 60  # 30 min slots\n",
    "radius_m = 250  # meters\n",
    "mean_lat = data['latitude'].mean()\n",
    "bandwidth_deg = radius_m / 111320  # approx degrees at equator\n",
    "\n",
    "homes = []\n",
    "\n",
    "valid_caid = set(user_metrics['caid'])\n",
    "\n",
    "for caid in valid_caid:\n",
    "    user_df = data.loc[data['caid'] == caid]\n",
    "\n",
    "    t = user_df['datetime_pdt'].astype(int) // 1e9\n",
    "    slots = (t // slot_size).astype(int)\n",
    "    user_df = user_df.assign(slot=slots)\n",
    "\n",
    "    superpings = (\n",
    "        user_df.groupby('slot')\n",
    "        .agg({'latitude': 'mean', 'longitude': 'mean'})\n",
    "        .dropna()\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "    if len(superpings) == 0:\n",
    "        continue\n",
    "\n",
    "    model = MeanShift(\n",
    "        bandwidth=bandwidth_deg,\n",
    "        bin_seeding=True,\n",
    "        min_bin_freq=2,\n",
    "        max_iter=50\n",
    "    )\n",
    "    model.fit(superpings)\n",
    "    home_lat, home_lon = model.cluster_center\n",
    "\n",
    "    homes.append({\n",
    "        'caid': caid,\n",
    "        'latitude': home_lat,\n",
    "        'longitude': home_lon,\n",
    "        'used_mean': model.used_mean\n",
    "    })\n",
    "\n",
    "homes_df = pd.DataFrame(homes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "bin_count = 100\n",
    "\n",
    "# Submetric 1: Avg Night Pings per Night\n",
    "axs[0].hist(user_metrics['avg_night_pings_per_night'].dropna(), bins=bin_count)\n",
    "axs[0].set_title('Avg Night Pings per Night')\n",
    "axs[0].set_xlabel('Avg Night Pings')\n",
    "axs[0].set_ylabel('Number of Users')\n",
    "\n",
    "# Submetric 2: Avg Bins per Night\n",
    "axs[1].hist(user_metrics['avg_bins_per_night'].dropna(), bins=bin_count)\n",
    "axs[1].set_title('Avg 30-min Bins per Night')\n",
    "axs[1].set_xlabel('Avg Bins')\n",
    "axs[1].set_ylabel('Number of Users')\n",
    "\n",
    "# Submetric 3: Unique Nights\n",
    "axs[2].hist(user_metrics['unique_nights'].dropna(), bins=bin_count)\n",
    "axs[2].set_title('Unique Nights with Data')\n",
    "axs[2].set_xlabel('Unique Nights')\n",
    "axs[2].set_ylabel('Number of Users')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.grid(True)\n",
    "    ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "bin_count = 50\n",
    "\n",
    "# --- Helper to add KDE ---\n",
    "def plot_hist_with_kde(ax, data, bins, title, xlabel):\n",
    "    # Histogram\n",
    "    counts, bins, patches = ax.hist(data, bins=bins, alpha=0.6, label='Histogram')\n",
    "\n",
    "    # KDE\n",
    "    kde = gaussian_kde(data)\n",
    "    x_vals = np.linspace(min(data), max(data), 1000)\n",
    "    kde_vals = kde(x_vals)\n",
    "    # Scale KDE to histogram height\n",
    "    kde_scaled = kde_vals * max(counts) / max(kde_vals)\n",
    "    ax.plot(x_vals, kde_scaled, color='red', label='KDE')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel('Number of Users')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "# --- Submetric 1: Log-space KDE ---\n",
    "data = user_metrics['avg_night_pings_per_night'].dropna()\n",
    "clip_max = 400\n",
    "data_clipped = np.clip(data, 0, clip_max)  # Clip huge outliers for clean plot\n",
    "\n",
    "# KDE in linear space, but clipped\n",
    "kde = gaussian_kde(data_clipped)\n",
    "x_vals = np.linspace(data_clipped.min(), data_clipped.max(), 1000)\n",
    "kde_vals = kde(x_vals)\n",
    "\n",
    "counts, bins, patches = axs[0].hist(data_clipped, bins=bin_count, alpha=0.6, label='Histogram')\n",
    "kde_scaled = kde_vals * max(counts) / max(kde_vals)\n",
    "axs[0].plot(x_vals, kde_scaled, color='red', label='KDE')\n",
    "\n",
    "axs[0].set_title(f'Avg Night Pings per Night (clipped at {clip_max})')\n",
    "axs[0].set_xlabel('Avg Night Pings')\n",
    "axs[0].set_ylabel('Number of Users')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "axs[0].set_yscale('log')\n",
    "\n",
    "# --- Submetric 2 ---\n",
    "plot_hist_with_kde(\n",
    "    axs[1],\n",
    "    user_metrics['avg_bins_per_night'].dropna(),\n",
    "    bins=bin_count,\n",
    "    title='Avg 30-min Bins per Night',\n",
    "    xlabel='Avg Bins'\n",
    ")\n",
    "axs[1].set_xlim(0,24)\n",
    "# --- Submetric 3 ---\n",
    "plot_hist_with_kde(\n",
    "    axs[2],\n",
    "    user_metrics['unique_nights'].dropna(),\n",
    "    bins=bin_count,\n",
    "    title='Unique Nights with Data',\n",
    "    xlabel='Unique Nights'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature to plot\n",
    "feature = 'avg_night_pings_per_night'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(user_metrics[feature], bins=100)\n",
    "ax.set_title('Distribution of Average Night Pings per Night')\n",
    "ax.set_xlabel('Average Night Pings per Night')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid(axis='y', zorder=0)\n",
    "\n",
    "for bar in ax.patches:\n",
    "    bar.set_zorder(2)\n",
    "\n",
    "ax.set_xlim(left=0, right=1000)\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_metrics['avg_night_pings_per_night'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to plot\n",
    "features = [\n",
    "    'total_night_pings',\n",
    "    'unique_nights',\n",
    "    'avg_night_pings_per_night',\n",
    "    'avg_bins_per_night',\n",
    "]\n",
    "\n",
    "# Plot each feature\n",
    "for feature in features:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(user_metrics[feature], bins=30)\n",
    "    ax.set_title(f'Distribution of {feature}')\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.grid(axis='y', zorder=0)\n",
    "    for bar in ax.patches:\n",
    "        bar.set_zorder(2)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
